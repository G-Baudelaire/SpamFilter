{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spam Classifier\n",
    "## Assignment Preamble\n",
    "Please ensure you carefully read all of the details and instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please post on the forum or ask a tutor well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "* Do not use global variables or rely on global state at all. It may lose you marks in the marking script.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. These assignments are designed to help improve your understanding first and foremost – the process of doing the assignment is part of *learning*. They are also used to assess your ability, and so you must uphold academic integrity. Submitting plagiarised work risks your entire place on your degree.\n",
    "\n",
    "**The pass mark for this assignment is 40%.** We expect that students, on average, will be able to produce a submission which gets a mark between 50-70% within the normal workload allocation for the unit, but this will vary depending on individual backgrounds. Please ask for help if you are struggling.\n",
    "\n",
    "## Getting Started\n",
    "Spam refers to unwanted email, often in the form of advertisements. In the literature, an email that is **not** spam is called *ham*. Most email providers offer automatic spam filtering, where spam emails will be moved to a separate inbox based on their contents. Of course this requires being able to scan an email and determine whether it is spam or ham, a classification problem. This is the subject of this assignment.\n",
    "\n",
    "This assignment has one part, worth 100% of the grade for this coursework.\n",
    "\n",
    "You will write a supervised learning based classifier to determine whether a given email is spam or ham. You must write and submit the code in this notebook. The training data is provided for you. You may use any classification method. Marks will be awarded primarily based on the accuracy of your classifier on unseen test data, but there are also marks for estimating how accurate you think your classifier will be.\n",
    "\n",
    "### Choice of Algorithm\n",
    "While the classification method is a completely free choice, the assignment folder includes [a separate notebook file](data/naivebayes.ipynb) which can help you implement a Naïve Bayes solution. If you do use this notebook, you are still responsible for porting your code into *this* notebook for submission. A good implementation should give a high  enough accuracy to get a good grade on this section (50-70%).\n",
    "\n",
    "You could also consider a k-nearest neighbour algorithm, but this may be less accurate. Logistic regression is another option that you may wish to consider.\n",
    "\n",
    "If you are looking to go beyond the scope of the unit, you might be interested in building something more advanced, like an artificial neural network. This is possible just using `numpy`, but will require significant self-directed learning. *Extensions like this are left unguided and are not factored into the unit workload estimates.*\n",
    "\n",
    "**Note:** you may use helper functions in libraries like `numpy` or `scipy`, but you **must not** import code which builds entire models for you. This includes but is not limited to use of libraries like `scikit-learn`, `tensorflow`, or `pytorch` – there will be plenty of opportunities for these libraries in later units. The point of this assignment is to understand code the actual algorithm yourself. ***If you are in any doubt about any particular library or function please ask a tutor.*** Submissions which ignore this will receive penalties or even zero marks.\n",
    "\n",
    "## Training Data\n",
    "The training data is described below and has 1000 rows. There is also a 500 row set of test data. These are functionally identical to the training data, they are just in a separate csv file to encourage you to split out your training and test data. You should consider how to best make use of all available data without overfitting, and to help produce an unbiased estimate for your classifier's accuracy.\n",
    "\n",
    "The cell below loads the training data into a variable called `training_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 0 0 ... 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Germain Jones\\AppData\\Local\\Temp\\ipykernel_4856\\1191426662.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(np.int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Your training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or ham `0`. The remaining 54 columns are _features_ that you will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned there is also a 500 row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam testing data set: (500, 55)\n",
      "[[1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Germain Jones\\AppData\\Local\\Temp\\ipykernel_4856\\3504744346.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(np.int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part One\n",
    "Write all of the code for your classifier below this cell. There is some very rough skeleton code in the cell directly below. You may insert more cells below this if you wish, but you must not duplicate any cells as this can break the grading script.\n",
    "\n",
    "### Submission Requirements\n",
    "Your code must provide a variable with the name `classifier`. This object must have a method called `predict` which takes input data and returns class predictions. The input will be a single $n \\times 54$ numpy array, your classifier should return a numpy array of length $n$ with classifications. There is a demo in the cell below, and a test you can run before submitting to check your code is working correctly.\n",
    "\n",
    "Your code must run on our test machine in under 30 seconds. If you wish to train a more complicated model (e.g. neural network) which will take longer, you are welcome to save the model's weights as a file and then load these in the cell below so we can test it. You must include the code which computes the original weights, but this must not run when we run the notebook – comment out the code which actually executes the routine and make sure it is clear what we need to change to get it to run. Remember that we will be testing your final classifier on additional hidden data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Type, Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Skalski, P (2018) Let’s code a Neural Network in plain NumPy. Available at: https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795 (Accessed: 16 April 2022)\n",
    "\n",
    "class ActivationFunction:\n",
    "    \"\"\"\n",
    "    Abstract class for an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Abstract method cannot be called.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function_derivative(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply derivative of activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Abstract method cannot be called.\")\n",
    "\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Relu activation function implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z_indexes)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function_derivative(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply derivative of activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        return (z_indexes > 0).astype(int)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        return np.divide(1, np.add(1, np.exp(-z_indexes)))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_function_derivative(z_indexes: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply derivative of activation function to activation values.\n",
    "        :param z_indexes: Activation values.\n",
    "        :return: Numpy array.\n",
    "        \"\"\"\n",
    "        sig = Sigmoid.apply_function(z_indexes)\n",
    "        return np.multiply(sig, np.subtract(1, sig))\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Object implementation of a layer within the neural network model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimension: int, output_dimension: int, activation: Type[ActivationFunction]) -> None:\n",
    "        self._input_dimension = input_dimension\n",
    "        self._output_dimension = output_dimension\n",
    "        self._activation = activation\n",
    "\n",
    "    def get_input_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        Getter for input_dimension.\n",
    "        :return: The number of input values or nodes in previous layer.\n",
    "        \"\"\"\n",
    "        return self._input_dimension\n",
    "\n",
    "    def get_output_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        Getter for output_dimension.\n",
    "        :return: The number of output values or nodes in this layer.\n",
    "        \"\"\"\n",
    "        return self._output_dimension\n",
    "\n",
    "    def get_activation(self) -> Type[ActivationFunction]:\n",
    "        \"\"\"\n",
    "        Getter for activation.\n",
    "        :return: The activation function used in this layer.\n",
    "        \"\"\"\n",
    "        return self._activation\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    \"\"\"\n",
    "    Abstract implementation of a loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_value(y: np.ndarray, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cost by comparing the expected output (y) with the actual output (y_hat).\n",
    "        :param y: Expected output.\n",
    "        :param y_hat: Actual output of the neural network.\n",
    "        :return: Cost calculation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Abstract method cannot be called.\")\n",
    "\n",
    "\n",
    "class BinaryCrossentropy(Loss):\n",
    "    \"\"\"\n",
    "    Binary Crossentropy function implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_value(y: np.ndarray, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cost by comparing the expected output (y) with the actual output (y_hat).\n",
    "        :param y: Expected output.\n",
    "        :param y_hat: Actual output of the neural network.\n",
    "        :return: Cost calculation.\n",
    "        \"\"\"\n",
    "        with np.errstate(divide='ignore'):\n",
    "            m = y_hat.shape[1]\n",
    "            p1 = np.divide(-1, m)\n",
    "            p2 = np.dot(y, np.log(y_hat).transpose())\n",
    "            p3 = np.dot(np.subtract(1, y), np.log(np.subtract(1, y_hat)).transpose())\n",
    "            cost = np.multiply(p1, np.add(p2, p3))\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "\n",
    "class Parameters:\n",
    "    \"\"\"\n",
    "    Object representation of the neural networks parameter, those being the weightings and biases for each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weightings: List[np.ndarray], biases: List[np.ndarray]) -> None:\n",
    "        self._weightings = weightings\n",
    "        self._biases = biases\n",
    "\n",
    "    def get_weightings(self) -> tuple[np.ndarray, ...]:\n",
    "        \"\"\"\n",
    "        Getter for deepcopy of weightings.\n",
    "        :return: Tuple of numpy arrays containing the weightings for each layer in the neural network.\n",
    "        \"\"\"\n",
    "        return tuple(weighting.copy() for weighting in self._weightings)\n",
    "\n",
    "    def get_biases(self) -> tuple[np.ndarray, ...]:\n",
    "        \"\"\"\n",
    "        Getter for deepcopy of biases.\n",
    "        :return: Tuple of numpy arrays containing the biases for each layer in the neural network.\n",
    "        \"\"\"\n",
    "        return tuple(bias.copy() for bias in self._biases)\n",
    "\n",
    "    def update_weightings(self, alpha: float, gradients_of_weightings: List[np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Update the weighting values after back propagation is completed.\n",
    "        :param alpha: Float of the learning rate.\n",
    "        :param gradients_of_weightings: List of the gradients of the weightings for each layer.\n",
    "        \"\"\"\n",
    "        for index in range(len(self._weightings)):\n",
    "            weighting = self._weightings[index]\n",
    "            gradient = gradients_of_weightings[index]\n",
    "            self._weightings[index] = np.subtract(weighting, np.multiply(alpha, gradient))\n",
    "\n",
    "    def update_biases(self, alpha: float, gradients_of_biases: List[np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Update the biases after back propagation is completed.\n",
    "        :param alpha: Float of the learning rate.\n",
    "        :param gradients_of_biases: List of the gradients of the biases for each layer.\n",
    "        \"\"\"\n",
    "        for index in range(len(self._biases)):\n",
    "            weighting = self._biases[index]\n",
    "            gradient = gradients_of_biases[index]\n",
    "            self._biases[index] = np.subtract(weighting, np.multiply(alpha, gradient))\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Class representation of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters: Parameters = None, loss: Loss = BinaryCrossentropy):\n",
    "        self._parameters = parameters\n",
    "        self._layers: List[Layer] = list()\n",
    "        self._cost_history = list()\n",
    "        self._accuracy_history = list()\n",
    "        self._loss = loss\n",
    "        self._init_training_attributes()\n",
    "\n",
    "    def add_layer(self, layer: Layer) -> None:\n",
    "        \"\"\"\n",
    "        Add layer to network, check that inputs to layer match outputs of previous layer.\n",
    "        :param layer: Layer object.\n",
    "        \"\"\"\n",
    "        if self._layers and self._layers[-1].get_output_dimension() != layer.get_input_dimension():\n",
    "            raise ValueError(\"Previous layer output dimension is not equal to new layer input dimension.\")\n",
    "        else:\n",
    "            self._layers.append(layer)\n",
    "\n",
    "    def add_loss(self, loss: Type[Loss]) -> None:\n",
    "        \"\"\"\n",
    "        Add loss function to network.\n",
    "        :param loss: Object representation of loss function.\n",
    "        \"\"\"\n",
    "        self._loss = loss\n",
    "\n",
    "    def _init_parameters(self, seed: int = 19) -> None:\n",
    "        \"\"\"\n",
    "        Initialise random parameters of weightings and biases for each layer.\n",
    "        :param seed: Optional seed for reproducibility.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        def initialise_weighting(layer: Layer) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Create 2 dimensional (output_dimension, input_dimension) Numpy array of random initial weightings.\n",
    "            :param layer: Corresponding layer of the neural network.\n",
    "            :return: Two dimensional Numpy array.\n",
    "            \"\"\"\n",
    "            return 0.1 * np.random.randn(layer.get_output_dimension(), layer.get_input_dimension()).astype(\n",
    "                np.longdouble)\n",
    "\n",
    "        def initialise_bias(layer: Layer) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Create 2 dimensional (output_dimension, 1) Numpy array of random initial biases.\n",
    "            :param layer: Corresponding layer of the neural network.\n",
    "            :return: Two dimensional Numpy array.\n",
    "            \"\"\"\n",
    "            return 0.1 * np.random.randn(layer.get_output_dimension(), 1).astype(np.longdouble)\n",
    "\n",
    "        weightings = [initialise_weighting(layer) for layer in self._layers]\n",
    "        biases = [initialise_bias(layer) for layer in self._layers]\n",
    "        self._parameters = Parameters(weightings, biases)\n",
    "\n",
    "    def _single_layer_forward_propagation(self, layer_index: int, prev_activation: np.ndarray,\n",
    "                                          activator_function: ActivationFunction) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform the activation values from the previous layer into z_index. Then run the given z_indexes through an\n",
    "        activator function. Additionally, cache the previous activation values and intermediate z_indexes.\n",
    "        :param layer_index: Index of the current layer being evaluated.\n",
    "        :param prev_activation: Activation values from the previous layer.\n",
    "        :param activator_function: Object representation of the activation function for the current layer.\n",
    "        :return: Y_hat of the current layer.\n",
    "        \"\"\"\n",
    "        curr_weighting = self._parameters.get_weightings()[layer_index]\n",
    "        curr_bias = self._parameters.get_biases()[layer_index]\n",
    "        curr_z_index = np.add(np.dot(curr_weighting, prev_activation), curr_bias)\n",
    "        self._z_index_cache[layer_index] = curr_z_index\n",
    "        self._activation_cache[layer_index] = prev_activation\n",
    "        return activator_function.apply_function(curr_z_index)\n",
    "\n",
    "    def _forward_propagation(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform propagation on each layer passing forward the activation values\n",
    "        :param inputs: Initial data.\n",
    "        :return: Numpy array of the networks outputs.\n",
    "        \"\"\"\n",
    "        curr_activation = inputs\n",
    "        for index, layer in enumerate(self._layers):\n",
    "            prev_activation = curr_activation\n",
    "            curr_activation = self._single_layer_forward_propagation(index, prev_activation, layer.get_activation())\n",
    "        return curr_activation\n",
    "\n",
    "    def _single_layer_backward_propagation(self, index: int, curr_d_activation: np.ndarray,\n",
    "                                           activation_function: Type[ActivationFunction]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Store the gradient of the weightings and biases after calculation for a layer of the neural network. Output\n",
    "        derivative of the previous activation values.\n",
    "        :param index: Index of the current layer.\n",
    "        :param curr_d_activation: Derivatives of the current activation values.\n",
    "        :param activation_function: Activation function used for this layer.\n",
    "        :return: Array of the derivatives of the previous activation values\n",
    "        \"\"\"\n",
    "        prev_activation = self._activation_cache[index]\n",
    "        curr_z_index = self._z_index_cache[index]\n",
    "        curr_weighting = self._parameters.get_weightings()[index]\n",
    "        m = prev_activation.shape[1]\n",
    "\n",
    "        curr_d_of_z_index = np.multiply(curr_d_activation, activation_function.apply_function_derivative(curr_z_index))\n",
    "        curr_d_of_weight = np.divide(np.dot(curr_d_of_z_index, prev_activation.transpose()), m)\n",
    "        curr_d_of_bias = np.divide(np.sum(curr_d_of_z_index, axis=1, keepdims=True), m)\n",
    "        prev_d_of_activation = np.dot(curr_weighting.transpose(), curr_d_of_z_index)\n",
    "        self._gradients_of_weightings[index] = curr_d_of_weight\n",
    "        self._gradients_of_biases[index] = curr_d_of_bias\n",
    "        return prev_d_of_activation\n",
    "\n",
    "    def _backward_propagation(self, y: np.ndarray, y_hat: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Perform backwards propagation on the neural network layer by layer from the end to start.\n",
    "        :param y: Expected output.\n",
    "        :param y_hat: Actual output of the neural network.\n",
    "        \"\"\"\n",
    "        y = np.reshape(y, y_hat.shape)\n",
    "\n",
    "        d1 = np.divide(y, y_hat, out=np.zeros(y.shape), where=(y_hat != 0))\n",
    "        s1 = np.subtract(1, y)\n",
    "        s2 = np.subtract(1, y_hat)\n",
    "        d2 = np.divide(s1, s2, out=np.zeros(s1.shape), where=(s2 != 0))  # Prevents divisions by zero\n",
    "        prev_d_of_activation = -np.subtract(d1, d2)\n",
    "        for index, layer in reversed(list(enumerate(self._layers))):\n",
    "            curr_d_of_activation = prev_d_of_activation\n",
    "            prev_d_of_activation = self._single_layer_backward_propagation(\n",
    "                index, curr_d_of_activation, layer.get_activation()\n",
    "            )\n",
    "\n",
    "    def _update(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the weighting and bias parameters.\n",
    "        :param learning_rate: Learning rate the neural network is currently being trained at.\n",
    "        \"\"\"\n",
    "        self._parameters.update_weightings(learning_rate, self._gradients_of_weightings)\n",
    "        self._parameters.update_biases(learning_rate, self._gradients_of_biases)\n",
    "\n",
    "    def _convert_probabilities_to_ones_and_zeros(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rounds the value of a probability to the nearest integer so every value is either a 0 or 1.\n",
    "        :param y_hat: Output of the neural network.\n",
    "        :return: Array of 0s and 1s\n",
    "        \"\"\"\n",
    "        return np.round(y_hat)\n",
    "\n",
    "    def _get_accuracy_value(self, y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Get the percentage of correct outputs.\n",
    "        :param y: Expected output.\n",
    "        :param y_hat: Actual output of the neural network.\n",
    "        :return: Float in the range [0, 1]\n",
    "        \"\"\"\n",
    "        return np.equal(y, self._convert_probabilities_to_ones_and_zeros(y_hat)).mean()\n",
    "\n",
    "    def _init_training_attributes(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialise arrays for storing numpy arrays while training the neural network.\n",
    "        \"\"\"\n",
    "        self._activation_cache = [None for i in range(len(self._layers))]\n",
    "        self._z_index_cache = [None for i in range(len(self._layers))]\n",
    "        self._gradients_of_weightings = [None for i in range(len(self._layers))]\n",
    "        self._gradients_of_biases = [None for i in range(len(self._layers))]\n",
    "\n",
    "    def _prepare_input_data(self, x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Transforms the arrays into a format for use in the training process.\n",
    "        :param x: Data for all entries.\n",
    "        :param y: The expected output for all entries.\n",
    "        :return: Data and expected output in a more useful arrangement.\n",
    "        \"\"\"\n",
    "        return x.transpose(), np.reshape(y, [1, -1])\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float) -> Tuple[\n",
    "        Parameters, List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        :param x: Data for all entries.\n",
    "        :param y: The expected output for all entries.\n",
    "        :param epochs: Number of iterations to train the neural network with.\n",
    "        :param learning_rate: Learning rate of the neural network.\n",
    "        :return: Parameter object, array of the costs, array of the network's accuracy.\n",
    "        \"\"\"\n",
    "        x, y = self._prepare_input_data(x, y)\n",
    "\n",
    "        if self._parameters is None:\n",
    "            self._init_parameters()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            self._init_training_attributes()\n",
    "            y_hat = self._forward_propagation(x)\n",
    "            self._cost_history.append(self._loss.get_cost_value(y, y_hat))\n",
    "            self._accuracy_history.append(self._get_accuracy_value(y, y_hat))\n",
    "            self._backward_propagation(y, y_hat)\n",
    "            self._update(learning_rate)\n",
    "\n",
    "        return self._parameters, self._cost_history, self._accuracy_history\n",
    "\n",
    "    def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict outputs from the given data. Raise error if neural network has no parameters object (has not been\n",
    "        trained).\n",
    "        :param data: Data of entries.\n",
    "        :return: Array outputting 0s and 1s\n",
    "        \"\"\"\n",
    "        if self._parameters is None:\n",
    "            raise NotImplementedError(\"Model has not been trained yet.\")\n",
    "\n",
    "        curr_activation = data.transpose()\n",
    "        for index, layer in enumerate(self._layers):\n",
    "            prev_activation = curr_activation\n",
    "            curr_weighting = self._parameters.get_weightings()[index]\n",
    "            curr_bias = self._parameters.get_biases()[index]\n",
    "            curr_z_index = np.add(np.dot(curr_weighting, prev_activation), curr_bias)\n",
    "            curr_activation = layer.get_activation().apply_function(curr_z_index)\n",
    "        return self._convert_probabilities_to_ones_and_zeros(curr_activation)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Read training and testing data and place them into a single array.\n",
    "    :return: Array of data to train on.\n",
    "    \"\"\"\n",
    "    training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    return np.concatenate((training_spam, testing_spam))\n",
    "\n",
    "\n",
    "def build_model(parameters=None) -> Model:\n",
    "    \"\"\"\n",
    "    Build a 5 layer model with Relu on the inner layers, Sigmoid on the final layer and Binary Crossentropy as the loss function.\n",
    "    :param parameters: Initialise model with weightings and biases to use a pretrained model.\n",
    "    :return: Model.\n",
    "    \"\"\"\n",
    "    model = Model(parameters)\n",
    "    layers = [Layer(54, 54, Relu), Layer(54, 54, Relu), Layer(54, 54, Relu), Layer(54, 54, Relu), Layer(54, 1, Sigmoid)]\n",
    "    for layer in layers:\n",
    "        model.add_layer(layer)\n",
    "    model.add_loss(BinaryCrossentropy)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model: Model, data: np.ndarray) -> Parameters:\n",
    "    \"\"\"\n",
    "    Train model the way the creator intended.\n",
    "    :param model: Untrained model.\n",
    "    :param data: Data to train on.\n",
    "    :return: Parameters object of the trained model.\n",
    "    \"\"\"\n",
    "    parameters, _, _ = model.train(data[:, 1:], data[:, 0], 750, 0.08)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def store_parameters(parameters: Parameters):\n",
    "    \"\"\"\n",
    "    Pickle the values of a Parameters object.\n",
    "    :param parameters: Parameter object.\n",
    "    \"\"\"\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    np.save(\"data/weightings.npy\", parameters.get_weightings(), allow_pickle=True)\n",
    "    np.save(\"data/biases.npy\", parameters.get_biases(), allow_pickle=True)\n",
    "\n",
    "\n",
    "def retrieve_parameters():\n",
    "    \"\"\"\n",
    "    Unpickle the values of a Parameters object.\n",
    "    :return: Parameter object of pre trained values.\n",
    "    \"\"\"\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    weightings = np.load(\"data/weightings.npy\", allow_pickle=True)\n",
    "    biases = np.load(\"data/biases.npy\", allow_pickle=True)\n",
    "    return Parameters(weightings, biases)\n",
    "\n",
    "\n",
    "def train_model_and_save_it_to_file():\n",
    "    \"\"\"\n",
    "    Train a model and pickle it's parameter values.\n",
    "    \"\"\"\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    data = get_data()\n",
    "    model = build_model()\n",
    "    parameters = train_model(model, data)\n",
    "    store_parameters(parameters)\n",
    "\n",
    "\n",
    "def create_classifier():\n",
    "    parameters = retrieve_parameters()\n",
    "    return build_model(parameters)\n",
    "\n",
    "\n",
    "# To create a classifier through training.\n",
    "# 1. Uncomment the function \"create_classifier_from_training\"\n",
    "# 2. Uncomment the line \"classifier = create_classifier_from_training(750, 0.08)\"\n",
    "# 3. Comment out \"classifier = create_classifier()\"\n",
    "# The values 750 and 0.08 are the epoch and learning rate I used to train the model, these can be changed. Learning rate\n",
    "# is expected to be in the range [0, 1) using other values would be considered undefined behaviour.\n",
    "# def create_classifier_from_training(epochs: int, learning_rate: float):\n",
    "#     np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "#     data = get_data()\n",
    "#     model = build_model()\n",
    "#     model.train(data[:, 1:], data[:, 0], epochs, learning_rate)\n",
    "#     return model\n",
    "\n",
    "# classifier = create_classifier_from_training(750, 0.08)\n",
    "classifier = create_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Accuracy Estimate\n",
    "In the cell below there is a function called `my_accuracy_estimate()` which returns `0.5`. Before you submit the assignment, write your best guess for the accuracy of your classifier into this function, as a percentage between `0.5` and `1`. So if you think you will get 80% of inputs correct, return the value `0.8`. This will form a small part of the marking criteria for the assignment, to encourage you to test your own code without bias – give the *most accurate* estimate you can.\n",
    "\n",
    "*Note* that there is no sense giving a value lower than `0.5` – if you are getting a score in this region, you can flip all of your predictions to get a better score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_accuracy_estimate():\n",
    "    return 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write all of the code for your classifier above this cell.\n",
    "\n",
    "### Testing Details\n",
    "Your classifier will be tested against some hidden data from the same source as the original. The accuracy (percentage of classifications correct) will be calculated, then benchmarked against common methods. At the very high end of the grading scale, your accuracy will also be compared to the best submissions from other students (in your own cohort and others!). Your estimate from the cell above will also factor in, and you will be rewarded for being close to your actual accuracy (overestimates and underestimates will be treated the same).\n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "The original skeleton code above classifies every row as ham, but once you have written your own classifier you can run this cell again to test it. So long as your code sets up a variable called `classifier` with a method called `predict`, the test code will be able to run. \n",
    "\n",
    "Of course you may wish to test your classifier in additional ways, but you *must* ensure this version still runs before submitting.\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "\n",
    "def tests():\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels) / test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")\n",
    "\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission Test\n",
    "The following cell tests if your notebook is ready for submission. **You must not skip this step!**\n",
    "\n",
    "Restart the kernel and run the entire notebook (Kernel → Restart & Run All). Now look at the output of the cell below. \n",
    "\n",
    "*If there is no output, then your submission is not ready.* Either your code is still running (did you forget to skip tests?) or it caused an error.\n",
    "\n",
    "As previously mentioned, failing to follow these instructions can result in a grade of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "129d240a6dfff29b30ef32256851036c",
     "grade": false,
     "grade_id": "cell-ce83a675162843d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All checks passed. When you are ready to submit, upload the notebook to the\n",
      "assignment page, without changing any filenames.\n",
      "\n",
      "If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\n"
     ]
    }
   ],
   "source": [
    "def submission_tests():\n",
    "    import sys\n",
    "    import pathlib\n",
    "\n",
    "    fail = False;\n",
    "\n",
    "    if not SKIP_TESTS:\n",
    "        fail = True;\n",
    "        print(\"You must set the SKIP_TESTS constant to True in the cell above.\")\n",
    "\n",
    "    p3 = pathlib.Path('./spamclassifier.ipynb')\n",
    "    if not p3.is_file():\n",
    "        fail = True\n",
    "        print(\"This notebook file must be named spamclassifier.ipynb\")\n",
    "\n",
    "    if \"create_classifier\" not in globals():\n",
    "        fail = True;\n",
    "        print(\"You must include a function called create_classifier.\")\n",
    "\n",
    "    if \"my_accuracy_estimate\" not in globals():\n",
    "        fail = True;\n",
    "        print(\n",
    "            \"You must include a function called my_accuracy_estimate which returns a hard-coded value between 0.5 and 1.\")\n",
    "    else:\n",
    "        if my_accuracy_estimate() == 0.5:\n",
    "            print(\"Warning:\")\n",
    "            print(\"You do not seem to have provided an accuracy estimate, it is set to 0.5.\")\n",
    "            print(\"This is the actually the worst possible accuracy – if your classifier\")\n",
    "            print(\"got 0.1 then it could invert its results to get 0.9!\")\n",
    "    print()\n",
    "\n",
    "    if fail:\n",
    "        sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "    else:\n",
    "        print(\"All checks passed. When you are ready to submit, upload the notebook to the\")\n",
    "        print(\"assignment page, without changing any filenames.\")\n",
    "        print()\n",
    "        print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\")\n",
    "\n",
    "\n",
    "submission_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "badbc892f539e03ad0acdb369f7e0993",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}